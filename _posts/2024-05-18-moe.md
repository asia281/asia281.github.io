---
layout: post
title: Mixture of Experts
date: 2024-05-18 20:00:00
description: MoE and novelties
tags: interview
categories: sample-posts
toc:
  sidebar: left
---

# MoE

 MoE model should achieve the same quality as its dense counterpart much faster during pretraining.


 ## Sparse MoE layers 
 
 are used instead of dense feed-forward network (FFN) layers. MoE layers have a certain number of “experts” (e.g. 8), where each expert is a neural network. In practice, the experts are FFNs, but they can also be more complex networks or even a MoE itself, leading to hierarchical MoEs!

A gate network or router, that determines which tokens are sent to which expert.