<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://asia281.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://asia281.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-15T16:11:12+00:00</updated><id>https://asia281.github.io/feed.xml</id><title type="html">Asia makes her first blog</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Causal discovery</title><link href="https://asia281.github.io/blog/2024/gradient-based-discovery-methods/" rel="alternate" type="text/html" title="Causal discovery"/><published>2024-01-15T00:00:00+00:00</published><updated>2024-01-15T00:00:00+00:00</updated><id>https://asia281.github.io/blog/2024/gradient-based-discovery-methods</id><content type="html" xml:base="https://asia281.github.io/blog/2024/gradient-based-discovery-methods/"><![CDATA[<h1 id="pc-algorithm">PC-algorithm</h1> <ol> <li>The PC algorithm starts with a complete graph. In the first step all node pairs are tested for conditional independence with conditional independence tests, suspect to a certain threshold. If those tests suggest conditional independence between pairs of nodes the edges are deleted from the complete graph.</li> <li>Based on the results of the conditional independence tests, orient the edges in the undirected graph. If two variables A and B are found to be conditionally independent given a set C, then the edge between A and B is removed or not oriented. If no conditional independence is found, the edge is oriented from A to B or from B to A.</li> <li>orientation rules to correctly orient edges around colliders to avoid inducing spurious relationships.</li> </ol> <h1 id="causal-discovery">Causal discovery</h1> <p>Discovering the graph G from samples of a joint distribution P is a fundamental problem in causality. While observational data alone is in general not sufficient to identify the DAG, interventional data can improve identifiability up to finding the exact graph. Finding the right DAG is challenging as the solution space grows super-exponentially with the number of variables. Commonly, methods that recover graphs from joint observational and interventional data are grouped into constraint-based and score-based approaches.</p> <h2 id="constraint-based">Constraint-based</h2> <p>These methods use conditional independence tests to identify causal relations.</p> <h2 id="score-based">Score-based</h2> <p>On the other hand, these methods search through the space of all possible causal structures with the goal of optimizing a specified metric. This metric, also referred to as score function, is usually a combination of how well the structure fits the data, for instance in terms of log-likelihood, as well as regularizers for encouraging sparsity. Since the search space of DAGs is super-exponential in the number of nodes, many methods rely on a greedy search, yet returning graphs in the true equivalence class</p> <h2 id="gradient-based-discovery-methods">Gradient-based discovery methods</h2> <p>are score-based methods that avoid the combinatorial greedy search over DAGs by using gradient-based methods. Thereby, the adjacency matrix is parameterized by weights that represent linear factors or probabilities of having an edge between a pair of nodes. The main challenge of such methods is how to limit the search space to acyclic graphs. One common approach is to view the search as a constrained optimization problem and deploy an augmented Lagrangian procedure to solve it (Zheng et al., 2018; 2020; Yu et al., 2019; Brouillard et al., 2020), including NOTEARS (Zheng et al., 2018) and DCDI (Brouillard et al., 2020). Alternatively, Ke et al. (2019) propose to use a regularization term penalizing cyclic graphs while allowing unconstrained optimization. However, the regularizer must be designed and weighted such that the correct, acyclic causal graph is the global optimum of the score function.</p> <h1 id="classical-discovery-methods">Classical discovery methods</h1> <h2 id="pc">PC</h2> <h2 id="fci">FCI</h2> <h1 id="gradient-based-discovery-methods-1">Gradient-based discovery methods</h1> <h2 id="enco-github">ENCO <a href="https://github.com/phlippe/ENCO" target="_blank" rel="noopener noreferrer">github</a></h2> <p>In ENCO, the structural parameters for an edge (i,j) are represented by two parameters ρi,j = [θi,j , γi,j ]. Intuitively, γi,j corresponds the existence of the edge, while θi,j = −θj,i is associated with the direction of the edge</p> <h2 id="dcdi-github">DCDI <a href="https://github.com/slachapelle/dcdi" target="_blank">github</a></h2> <p>It combines constraint and score-based approaches. Among these, IGSP is a method that optimizes a score based on conditional independence tests. Contrary to GIES, this method has been shown to be consistent under the faithfulness assumption.</p> <h2 id="deci">DECI</h2> <h2 id="dibs">DiBS</h2> <h2 id="sdi">SDI</h2>]]></content><author><name></name></author><category term="sample-posts"/><category term="git"/><summary type="html"><![CDATA[PC-algorithm]]></summary></entry><entry><title type="html">Markov equivalence class</title><link href="https://asia281.github.io/blog/2024/markov-equvalence-class/" rel="alternate" type="text/html" title="Markov equivalence class"/><published>2024-01-11T00:00:00+00:00</published><updated>2024-01-11T00:00:00+00:00</updated><id>https://asia281.github.io/blog/2024/markov-equvalence-class</id><content type="html" xml:base="https://asia281.github.io/blog/2024/markov-equvalence-class/"><![CDATA[<h2 id="d-separation">d-separation</h2> <p>d-separation is a criterion for deciding, from a given a causal graph, whether a set X of variables is independent of another set Y, given a third set Z. The idea is to associate “dependence” with “connectedness” (i.e., the existence of a connecting path) and “independence” with “unconnected-ness” or “separation”. The only twist on this simple idea is to define what we mean by “connecting path”, given that we are dealing with a system of directed arrows in which some vertices (those residing in Z) correspond to measured variables, whose values are known precisely. To account for the orientations of the arrows we use the terms “d-separated” and “d-connected” (d connotes “directional”).</p> <p>We start by considering separation between two singleton variables, x and y; the extension to sets of variables is straightforward (i.e., two sets are separated if and only if each element in one set is separated from every element in the other).</p> <h3 id="rules">Rules:</h3> <ul> <li>x and y are d-connected if there is an unblocked path between them.</li> <li>If a collider is a member of the conditioning set Z, or has a descendant in Z, then it no longer blocks any path that traces this collider.</li> <li>x and y are d-connected, conditioned on a set Z of nodes, if there is a collider-free path between x and y that traverses no member of Z. If no such path exists, we say that x and y are d-separated by Z, We also say then that every path between x and y is “blocked” by Z.</li> </ul> <h2 id="markov-equivalence">Markov equivalence</h2> <p>When exactly the same set of d-separation relations hold in two directed graphs, no matter whether respectively cyclic or acyclic, we say that they are Markov equivalent.</p> <p>Two DAGs \(G_1, G_2\) are Markov equivalent if and only if</p> <ol> <li>\(G_1\) and \(G_2\) contain the same vertices</li> <li>There is an edge between A and B in \(G_1\) iff there is an edge between A and B in \(G_2\);</li> <li>\(G_1\) and \(G_2\) have the same unshielded colliders. These conditions imply a fourth condition:</li> <li>\(G_1\) and \(G_2\) have the same unshielded noncolliders.</li> </ol> <pre><code class="language-mermaid">sequenceDiagram
    participant A
    participant B
    participant C
    participant D
    A--&gt;&gt;B;
    B--&gt;&gt;C;
    A--&gt;&gt;C;
</code></pre> <h2 id="future-reading">Future reading:</h2> <ul> <li><a href="https://arxiv.org/pdf/1104.2808.pdf" target="_blank">Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs</a></li> </ul>]]></content><author><name></name></author><category term="causality"/><summary type="html"><![CDATA[Why we can't discover]]></summary></entry><entry><title type="html">Causal interference</title><link href="https://asia281.github.io/blog/2024/do-calculus/" rel="alternate" type="text/html" title="Causal interference"/><published>2024-01-02T00:00:00+00:00</published><updated>2024-01-02T00:00:00+00:00</updated><id>https://asia281.github.io/blog/2024/do-calculus</id><content type="html" xml:base="https://asia281.github.io/blog/2024/do-calculus/"><![CDATA[<h2 id="rules">Rules</h2> <ol> <li> \[P(y | do(t),z,w) = P(y | do(t),w) if Y \perp _{G_T} Z | T,W\] </li> </ol> <p>It’s a generalization of d-separation to interventional distributions.</p> \[P(y|z,w)=P(y|w) if Y \perp _G Z|W\] <ol> <li> \[P(y | do(t),do(z),w) = P(y | do(t),z,w) if Y ?GT,Z Z | T,W\] </li> </ol> <p>It’s a generalization of backdoor adjustment/criterion</p> <table> <tbody> <tr> <td>P(y</td> <td>do(z),w)=P(y</td> <td>z,w) if Y ?GZ Z</td> <td>W</td> </tr> </tbody> </table> <ol> <li> \[P(y | do(t),do(z),w) = P(y | do(t),w) if Y \perp GT,Z(W) Z | T,W\] </li> </ol> <p>where $Z(W)$ denotes the set of nodes of $Z$ that aren’t ancestors of any node of $W$ in $G_T$.</p> \[P(y|do(z),w)=P(y|w) if Y ?GZ(W) Z|W\] <h2 id="examples">Examples</h2> <p>\(P_G(y|do(x)) = p(y)\) if $Y$ is a non-descendant of $X$.</p> <p>If $Y$ is not a parent of $X$ then \(P_G (y | do(X = \hat{x})) = \sum_{pa_X} P(y | \hat{x}, pa_X ) p(pa_X )\)</p> <table> <tbody> <tr> <td>Whenever we can compute the marginalized intervention distribution p(y</td> <td>do(X = xˆ)) by a summation 􏰈z p(y</td> <td>xˆ, z) p(z) as in (2), we call the set Z a valid adjustment set for the intervention $Y</td> <td>do(X)$. We see that $Z = PA^G_X$ is a valid adjustment set for Y</td> <td>do(X) (for any Y ).</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="causality"/><summary type="html"><![CDATA[Causal interference]]></summary></entry><entry><title type="html">Causal interference</title><link href="https://asia281.github.io/blog/2024/causal-basics/" rel="alternate" type="text/html" title="Causal interference"/><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>https://asia281.github.io/blog/2024/causal-basics</id><content type="html" xml:base="https://asia281.github.io/blog/2024/causal-basics/"><![CDATA[<h3 id="sources">Sources</h3> <p>If you want to start your adventure with causality, I recommend to watch the videos from <a href="https://www.bradyneal.com/causal-inference-course" target="_blank">Causal Inference Course</a>. They honestly have the best explanations of causal theory I’ve ever seen.</p> <p>What is the primary question of causal interference?</p> <h1 id="why-is-association-not-causation">Why is association not causation?</h1> <h2 id="how-to-allow-identifiability-computing-from-purely-statistical-quantities">How to allow identifiability? (computing from purely statistical quantities)</h2> <p>Ignorability: (Y(1), Y(0)) independent T. Intuition: we ignore all missing data and compute statistics based on data we have.</p> \[E(Y(1)) - E(Y(0)) = E(Y(1)|T = 1) - E(Y(0) | T = 0) = E(Y|T = 1) - E(Y|T = 0)\] <p>Confounding disappears after adding ignorability.</p> <h1 id="exchangeability">Exchangeability:</h1> <p>We can swap the groups and we’ll get the same expected value. We can write it as:</p> \[E(Y(1) | T = 1) = E(Y(1) | T = 0)= E(Y(1))\] <p>Potential outcome Y is independent from treatment.</p> <h2 id="ranomized-control-trial-rct">Ranomized control trial (RCT)</h2> <p>Forcing people in groups to change their trial type in order to make group random. It removes confounders. Graphical interpretation: removing an edge. It makes a graph satisfy backdoor criterion. Some</p> <table> <tbody> <tr> <td>If we have covariate balance ($P(X</td> <td>T = 0) = P(X</td> <td>T = 1)$), then assosiation is causation ($P(X</td> <td>do(T)) = P (X</td> <td>T)$).</td> </tr> </tbody> </table> <h2 id="backdoor">Backdoor</h2> <h1 id="criterion">Criterion</h1> <p>To establish a causal relationship between X and Y, you need to collectively block (condition on) all backdoor paths that could introduce confounding. The backdoor criterion states that a variable set $Z$ satisfies the criterion if and only if:</p> <ol> <li>$Z$ blocks all backdoor paths from $X$ to $Y$.</li> <li>There are no colliders on the backdoor paths that are in the set $Z$.</li> </ol> <p>We can ilustrate if as follows: <img src="/assets/img/openbackdoor.svg" alt="Backdoor"/></p> <p>Let’s do some examples. Let’s consider the following graph: <img src="/assets/img/biasexamples.svg" alt="Backdoor-example"/></p> <ol> <li>$X \perp T$ Y is a collider that hasn’t been conditioned on.</li> <li>$X \nperp T | Y$ Y is a collider that has been conditioned on</li> <li>$X \nperp T | W$ W has been conditioned on, it is a descendant of a collider</li> <li>$Y \perp G$</li> <li>$Y \nperp G | U$ U is a collider that has been conditioned on</li> <li>$Y \perp G | U, T$ T is a confounder that has been conditioned on</li> </ol> <h1 id="adjustment">Adjustment</h1> <h2 id="frontdoor-criterion">Frontdoor criterion</h2> <p>Like the backdoor criterion, the front door criterion is used to identify and estimate causal relationships between variables. It is applicable when there is an unobserved or latent variable M that acts as an intermediate step between the X and the Y.</p> <p>A set of variables \(M\) satisfies the frontdoor criterion relative to \(T\) and \(Y\) if the following are true:</p> <ol> <li>\(M\) completely mediates the effectof \(T\) on \(Y\) (i.e. all causal paths from T to Y go through M).</li> <li>There is no unblocked backdoor path from T to M.</li> <li>All backdoor paths from M to Y are blocked by T.</li> </ol> <h1 id="adjustment-1">Adjustment</h1> <p>If \(Y, M, T\) satisfy criterion and we have positivity, then: \(P(y|do(t)) = \sum_m P(m|t) \sum _{t'} P(y|m, t')P(t')\)</p> <h1 id="other-criterions">Other criterions</h1> <p><strong>Necessary criterion:</strong> For each backdoor path from \(T\) to any child \(M\) of \(T\) that is an ancestor of \(Y\), we must block the path. It’s not sufficient criterion.</p> <p><strong>Unconfounded children criterion</strong>: All backdoor paths from the treatment variable T to all of its children that are ancestors of Y with a single conditioning set must be block. Sufficient when T is a single variable.</p> <p><img src="/assets/img/unconfounded_child.png" alt="Backdoor-example"/></p> <h1 id="currently-we-usually-try-to-solve-one-of-the-following-problems">Currently, we usually try to solve one of the following problems:</h1> <ol> <li> <p><strong>Causal inference</strong>: How much would some specific variables (features or the label) change if we manipulate the value of another specific variable?</p> </li> <li> <p><strong>Causal discovery</strong>: By modifying the value of which variables could we change the value of another variable?</p> </li> </ol> <h1 id="propensity-score">Propensity score</h1> \[e(w) = P(T=1 | W)\] <p>e is only 1-dimentional</p> <p>Given positivity:</p> \[(Y(0), Y(1)) \perp T | W \arrow (Y(0), Y(1)) \perp T | e(W)\]]]></content><author><name></name></author><category term="causality"/><summary type="html"><![CDATA[Causal interference]]></summary></entry></feed>